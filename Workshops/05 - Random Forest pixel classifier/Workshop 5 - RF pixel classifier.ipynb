{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 5 - A random forest pixel classifier\n",
    "\n",
    "This week we had a quick look at the usage of ML algorithms in image analysis.\n",
    "\n",
    "The lectures gave a very broad and general overview of different ML algorithms.\n",
    "\n",
    "In this workshop we will take a much more practical approach and build a pixel classifier using Random Forests (RF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "Random Forests is a supervised learning algorithm that works by generating a large number of classification trees, that work on a bootstrapped sample of the training observations and of the training features.\n",
    "\n",
    "For the context of this workshop, you don't need to exactly know how RF works, but if you want to refresh your mind, I have attached the slides from the IBMS3 lecture you should have all had last year.\n",
    "\n",
    "### Aim of the task\n",
    "\n",
    "For this workshop we want to train a pixel classifier that can classify pixels in an image into a certain number of classes.\n",
    "\n",
    "We will provide one image and label just a few pixels of different classes, and hopefully RF will work its magic and classify the rest! We will then try to classify new images.\n",
    "\n",
    "We will try a few different things and try to see how good our classifier is, by comparing our results with ground truth segmentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "I have included 4 images of DAPI-stained nuclei (source: Florian Kromp et al. 2020, [\"An annotated fluorescence image dataset for training nuclear segmentation methods\"](https://www.nature.com/articles/s41597-020-00608-w), CC0) and corresponding ground truth labels. Note that, while the ground truth labels show instance segmentations, we will only do a semantic segmentation here. We will therefore convert the ground truth labels to a binary mask.\n",
    "\n",
    "Let's start by plotting the images and their GT labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# The names of the folders where our files are stored\n",
    "# The file naming is the same in the two folders\n",
    "GT_folder = \"GT\"\n",
    "img_folder = \"Images\"\n",
    "\n",
    "# Read the filenames of the images\n",
    "filenames = os.listdir(img_folder)\n",
    "print(filenames)\n",
    "\n",
    "# Loop through the files, read the images and GT data (remember to binarise it!) and plot them\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now just mark a few pixels as belonging to a nucleus or the background.\n",
    "We could manually draw a mask but for the sake of simplicity we'll just select a crop of the image.\n",
    "\n",
    "Let's find a 200x200 square in the image containing both nuclei and background.\n",
    "We will start working on the first image (`Normal1.tif`).\n",
    "\n",
    "In reality, you might do something like this:\n",
    "\n",
    "![Example of marking a few pixels](Normal1_marked_pixels.jpg)\n",
    "\n",
    "Here I marked a few background pixels in red and a few nucleus pixels in blue.\n",
    "\n",
    "For simplicity, here we are going to use a cropped version of the image, but you could try and save a mask with those strokes and fetch the intensity and labels of the non zero pixels to use as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first image and the GT (make sure to binarise it!)\n",
    "# img = _____\n",
    "# gt = _____\n",
    "\n",
    "# The coordinates of our training region\n",
    "# We can just grab GT labels, but in a real-world situation we would probably have to \n",
    "# manually \"paint\" over the image to set the labels \n",
    "x, y, width, height = (500, 500, 200, 200)\n",
    "training = img[y:y+height, x:x+width]\n",
    "\n",
    "# Plot the image, the cropped area and the GT labels\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input features need to have observations as rows and features in columns. We only have one feature here (intensity) and 100x100=10000 observations. We can therefore reshape our training data into a single column.\n",
    "\n",
    "The training labels can be reshaped into a single row.\n",
    "\n",
    "Remember that this 200x200 square is the only data that our classifier will see during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = training.reshape(-1, 1) # -1 means the number of rows will be determined automatically\n",
    "y_train = gt[y:y+height, x:x+width].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier\n",
    "\n",
    "We can finally create a Random Forest classifier.\n",
    "\n",
    "Import `RandomForestClassifier` from the `sklearn.ensemble` module and initialise it with some parameters of your choice. The number of trees is passed through the `n_estimators` parameter, and you can set the maximum depth of the trees with the `max_depth` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_estimators = ___\n",
    "max_depth = ___\n",
    "\n",
    "# Initialise the classifier with the hyperparameters of our choice. \n",
    "# You can set the random state for reproducibility\n",
    "rf = RandomForestClassifier(_____)\n",
    "\n",
    "# Now train the classifier with the pixels of the nuclei and the background\n",
    "rf = rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the classifier is trained, we can use it to classify all the pixels in the image! Again, we reshape the image into a single column, then reshape back the results into a 2D image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction \n",
    "label_pred = rf.predict(img.ravel().reshape(-1,1))\n",
    "label_pred = label_pred.reshape(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, print the results and compare with GT!\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be pretty good! Even just by using a single feature (the intensity) the classifier does a good job. This is because the two classes are clearly separable by the intensity. \n",
    "\n",
    "The classifier is not perfect, though, failing especially at the edges of cells. \n",
    "Let's see how this performs on the other images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the files and predict the labels with the classifier.\n",
    "# Plot the results.\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do you see anything worth noting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy metrics\n",
    "\n",
    "Before we move onto improving our classifier, we need a way to assess the goodness of our model.\n",
    "\n",
    "### Pixel accuracy \n",
    "\n",
    "A simple solution is to count the percentage of correctly classified pixels. This is called **pixel accuracy** and is defined as:\n",
    "\n",
    "$\\text{Pixel accuracy} = \\Large \\frac{\\text{correctly classified pixels}}{\\text{total pixels}}$\n",
    "\n",
    "This is a very simple metric, but it can be **extremely misleading**. Say that 95% of your image is background and your classifier predicts that every single pixel is background... you still end up with 95% accuracy, which is obviously not great in this case!\n",
    "\n",
    "*Can we do better?*\n",
    "\n",
    "Two commonly used metrics for evaluating segmentation are the **IoU** (intersection over union, or Jaccard coefficient) and the **F1 score** (or Dice coefficient).\n",
    "\n",
    "### IoU\n",
    "\n",
    "**IoU** is defined as:\n",
    "\n",
    "$IoU = \\Large \\frac{\\text{intersection of pixels}}{\\text{union of pixels}}$\n",
    "\n",
    "This is calculated for each class and then averaged.\n",
    "\n",
    "The *intersection* is the number of pixels that are classified in the same way (e.g. background) in both GT and prediction.\n",
    "The *union* is the total number of non-overlapping pixels, classified in some way (e.g. background) in GT or prediction.\n",
    "\n",
    "\n",
    "### F1 score\n",
    "\n",
    "The F1 score is defined as:\n",
    "\n",
    "$F1 = \\Large \\frac{2 \\times \\text{intersection of pixels}}{\\text{union of pixels} + \\text{intersection of pixels}}$\n",
    "\n",
    "\n",
    "Both the Iou and the F1 score range from 0 to 1; they are very similar, and always correlated. However, they might give slightly different results depending on the problem (see [this discussion on the topic](https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou/276144)).  \n",
    "\n",
    "\n",
    "So, let's define three functions to calculate these metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(y_true, y_pred):\n",
    "    \"\"\"Returns pixel accuracy of the predicted labels\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): The ground truth labels\n",
    "        y_pred (np.array): The predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: The pixel accuracy, ranging from 0 to 1\n",
    "    \"\"\"\n",
    "    # Ensure inputs are the same shape\n",
    "    assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
    "\n",
    "    # Calculate the number of pixels that are correctly labelled\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "def IoU(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Returns the Intersection over Union (IoU) of the predicted labels\n",
    "    \n",
    "        Args:\n",
    "            y_true (np.array): The ground truth labels\n",
    "            y_pred (np.array): The predicted labels\n",
    "    \n",
    "        Returns:\n",
    "            float: The IoU, ranging from 0 to 1\n",
    "        \"\"\"\n",
    "\n",
    "    # Ensure inputs are the same shape\n",
    "    assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
    "\n",
    "    intersection = np.logical_and(y_true, y_pred)\n",
    "    union = np.logical_or(y_true, y_pred)\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "    return iou_score\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Returns the F1 score of the predicted labels\n",
    "\n",
    "        Args:\n",
    "            y_true (np.array): The ground truth labels\n",
    "            y_pred (np.array): The predicted labels\n",
    "    \n",
    "        Returns:\n",
    "            float: The F1 score, ranging from 0 to 1\n",
    "        \"\"\"\n",
    "\n",
    "    # Ensure inputs are the same shape\n",
    "    assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
    "\n",
    "    intersection = np.logical_and(y_true, y_pred)\n",
    "    union = np.logical_or(y_true, y_pred)\n",
    "    \n",
    "    F1 = (2 * np.sum(intersection)) / (np.sum(union) + np.sum(intersection))\n",
    "\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now loop through the files and calculate the accuracy, IoU, \n",
    "# and F1 score for each image\n",
    "# Store the results of the predictions in arrays, so that you can plot it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the results you just got. Are they in line with what you observed by looking at the prediction results? What do the segmentation accuracy plots tell you? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra exercises\n",
    "\n",
    "1. Try to add new training data from another image. Does that improve the results? \n",
    "\n",
    "We only used a single feature (intensity) to train the classifier. This is OK here because intensity discriminates well nuclei and background, however, you could improve classification by passing more features to the classifier. For example you could add the gradient magnitude, or the entropy, or GLCM features! You could also calculate those features on a series of gaussian-filtered versions of the image (with different $\\sigma$), to get information from neighbouring pixels as well. \n",
    "\n",
    "2. Try segmenting the `skin.jpg` image similarly to what we have done above! I am not providing GT for this image, but you can evaluate the results visually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Well done, you made it to the end of the workshop 5! I hope you enjoyed it; if you have any questions, please [ask](mailto:nicola.romano@ed.ac.uk)!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af6b460c36dc28ebfac4d0bba8d4a91774bd4ef791c3ae938522999fae061eb7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('bia4_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
